{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8780b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0acc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def grab_frame():\n",
    "    ret, frame = cap.read()\n",
    "    # Convert the image from BGR to RGB format\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55629f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9587eed6c7604d3d8271d219709a826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an image widget\n",
    "image_widget = widgets.Image(format='jpeg')\n",
    "\n",
    "# Define a function to display the webcam feed\n",
    "def display_image(frame):\n",
    "    # Convert the frame to JPEG\n",
    "    _, frame = cv2.imencode('.jpeg', frame)\n",
    "    image_widget.value = frame.tobytes()\n",
    "\n",
    "# Update the image widget with the current frame\n",
    "frame = grab_frame()\n",
    "display_image(frame)\n",
    "\n",
    "# Display the image widget\n",
    "display(image_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43451522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam feed stopped.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Update the webcam feed every 100ms\n",
    "try:\n",
    "    while True:\n",
    "        frame = grab_frame()\n",
    "        display_image(frame)\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    # Release the webcam when interrupted\n",
    "    cap.release()\n",
    "    print(\"Webcam feed stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b927f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unsupported distribution!\r\n",
      "# Check https://nvidia.github.io/libnvidia-container\r\n"
     ]
    }
   ],
   "source": [
    "!curl -sL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/nvidia.gpg\n",
    "!curl -sL https://nvidia.github.io/libnvidia-container/$DISTRO/$ARCH/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/libnvidia-container.list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a9439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://ports.ubuntu.com/ubuntu-ports bionic InRelease\n",
      "Hit:2 http://packages.ros.org/ros/ubuntu bionic InRelease               \n",
      "Get:3 http://ports.ubuntu.com/ubuntu-ports bionic-updates InRelease [88.7 kB]\n",
      "Get:4 http://ports.ubuntu.com/ubuntu-ports bionic-backports InRelease [83.3 kB]\n",
      "Get:5 http://ports.ubuntu.com/ubuntu-ports bionic-security InRelease [88.7 kB]\n",
      "Fetched 261 kB in 2s (160 kB/s)                              \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "E: Unable to locate package nvidia-docker2\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install nvidia-docker2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f07e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "E: Unable to locate package python3-libnvinfer-dev\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install python3-libnvinfer-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83653f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist/jp/v45\n",
      "Collecting tensorflow<2\n",
      "  Downloading https://developer.download.nvidia.com/compute/redist/jp/v45/tensorflow/tensorflow-1.15.5%2Bnv21.6-cp36-cp36m-linux_aarch64.whl (230.8 MB)\n",
      "     |################################| 230.8 MB 10 kB/s              \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting h5py<=2.10.0\n",
      "  Downloading h5py-2.10.0.tar.gz (301 kB)\n",
      "     |################################| 301 kB 5.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py>=0.9.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     |################################| 126 kB 8.4 MB/s            \n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5.zip (5.4 MB)\n",
      "     |################################| 5.4 MB 8.4 MB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     |################################| 57 kB 2.1 MB/s             \n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "     |################################| 50 kB 2.8 MB/s             \n",
      "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "     |################################| 3.8 MB 11.8 MB/s            \n",
      "\u001b[?25hCollecting astor==0.8.1\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |################################| 42 kB 568 kB/s             \n",
      "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "     |################################| 503 kB 18.1 MB/s            \n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.15.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (75 kB)\n",
      "     |################################| 75 kB 1.9 MB/s             \n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_aarch64.whl (63.7 MB)\n",
      "     |################################| 63.7 MB 28 kB/s              \n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |################################| 65 kB 1.7 MB/s             \n",
      "\u001b[?25hCollecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
      "     |################################| 162 kB 15.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow<2) (1.11.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2) (0.37.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "     |################################| 97 kB 2.4 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2) (59.6.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     |################################| 289 kB 12.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (4.8.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (3.6.0)\n",
      "Building wheels for collected packages: h5py, numpy, termcolor\n",
      "  Building wheel for h5py (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-6xst6gpn\n",
      "       cwd: /tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/\n",
      "  Complete output (66 lines):\n",
      "  /usr/local/lib/python3.6/dist-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
      "    SetuptoolsDeprecationWarning,\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-aarch64-3.6\n",
      "  creating build/lib.linux-aarch64-3.6/h5py\n",
      "  copying h5py/version.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "  copying h5py/__init__.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "  copying h5py/ipy_completer.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "  copying h5py/highlevel.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "  copying h5py/h5py_warnings.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "  creating build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/__init__.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/base.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/filters.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/group.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/attrs.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/datatype.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/files.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/dataset.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/dims.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/vds.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/compat.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/selections2.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  copying h5py/_hl/selections.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "  creating build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_attribute_create.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_file_image.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_attrs.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/__init__.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_h5pl.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_dataset_swmr.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_attrs_data.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/common.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_dims_dimensionproxy.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_h5p.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_datatype.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_filters.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_objects.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_dimension_scales.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_h5f.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_file.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_h5d_direct_chunk.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_h5t.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_group.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_selections.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_slicing.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_dtype.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_deprecation.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_completions.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_dataset.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_base.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_h5.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_dataset_getitem.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_threads.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  copying h5py/tests/test_file2.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "  creating build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "  copying h5py/tests/test_vds/__init__.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "  copying h5py/tests/test_vds/test_virtual_source.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "  copying h5py/tests/test_vds/test_highlevel_vds.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "  copying h5py/tests/test_vds/test_lowlevel_vds.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "  running build_ext\n",
      "  Loading library to get version: libhdf5.so\n",
      "  error: libhdf5.so: cannot open shared object file: No such file or directory\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for h5py\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for h5py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for numpy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for numpy: filename=numpy-1.18.5-cp36-cp36m-linux_aarch64.whl size=12788771 sha256=21bf616a238764348ebae96f02351ba78d02b6894c560f3531188d32071a16d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/95/fa/4700540e8376bb1def7bf28c60adb71a0ce28d3cf657f6bafa\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=d644d9ebef4d7bbe542e198bce46a762c10a30a92c8dd265b8505a11d84ddc01\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built numpy termcolor\n",
      "Failed to build h5py\n",
      "Installing collected packages: numpy, werkzeug, protobuf, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, astunparse, astor, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.13.3\n",
      "    Uninstalling numpy-1.13.3:\n",
      "      Successfully uninstalled numpy-1.13.3\n",
      "    Running setup.py install for h5py ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-nlhd37vy/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6/h5py\n",
      "         cwd: /tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/\n",
      "    Complete output (68 lines):\n",
      "    /usr/local/lib/python3.6/dist-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
      "      SetuptoolsDeprecationWarning,\n",
      "    running install\n",
      "    /usr/local/lib/python3.6/dist-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      setuptools.SetuptoolsDeprecationWarning,\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-aarch64-3.6\n",
      "    creating build/lib.linux-aarch64-3.6/h5py\n",
      "    copying h5py/version.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "    copying h5py/__init__.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "    copying h5py/ipy_completer.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "    copying h5py/highlevel.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "    copying h5py/h5py_warnings.py -> build/lib.linux-aarch64-3.6/h5py\n",
      "    creating build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/__init__.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/base.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/filters.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/group.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/attrs.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/datatype.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/files.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/dataset.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/dims.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/vds.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/compat.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/selections2.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    copying h5py/_hl/selections.py -> build/lib.linux-aarch64-3.6/h5py/_hl\n",
      "    creating build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_attribute_create.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_file_image.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_attrs.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/__init__.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_h5pl.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_dataset_swmr.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_attrs_data.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/common.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_dims_dimensionproxy.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_h5p.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_datatype.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_filters.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_objects.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_dimension_scales.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_h5f.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_file.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_h5d_direct_chunk.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_h5t.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_group.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_selections.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_slicing.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_dtype.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_deprecation.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_completions.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_dataset.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_base.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_h5.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_dataset_getitem.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_threads.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    copying h5py/tests/test_file2.py -> build/lib.linux-aarch64-3.6/h5py/tests\n",
      "    creating build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "    copying h5py/tests/test_vds/__init__.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "    copying h5py/tests/test_vds/test_virtual_source.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "    copying h5py/tests/test_vds/test_highlevel_vds.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "    copying h5py/tests/test_vds/test_lowlevel_vds.py -> build/lib.linux-aarch64-3.6/h5py/tests/test_vds\n",
      "    running build_ext\n",
      "    Loading library to get version: libhdf5.so\n",
      "    error: libhdf5.so: cannot open shared object file: No such file or directory\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-4udy3z74/h5py_f1d04a01562540efba2a6b480e0ffad6/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-nlhd37vy/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6/h5py Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v46 'tensorflow<2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# Download and extract the pre-trained model\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'\n",
    "MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME\n",
    "PATH_TO_MODEL_TAR = os.path.join('models', MODEL_TAR_FILENAME)\n",
    "PATH_TO_CKPT = os.path.join('models', os.path.join(MODEL_NAME, 'checkpoint/'))\n",
    "PATH_TO_CFG = os.path.join('models', os.path.join(MODEL_NAME, 'pipeline.config'))\n",
    "\n",
    "if not os.path.exists(PATH_TO_CKPT):\n",
    "    os.makedirs(PATH_TO_CKPT)\n",
    "\n",
    "if not os.path.exists(PATH_TO_CFG):\n",
    "    os.makedirs(PATH_TO_CFG)\n",
    "\n",
    "urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)\n",
    "tar_file = tarfile.open(PATH_TO_MODEL_TAR)\n",
    "tar_file.extractall('models')\n",
    "tar_file.close()\n",
    "\n",
    "os.remove(PATH_TO_MODEL_TAR)\n",
    "\n",
    "# Download the label map\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "LABELS_DOWNLOAD_BASE = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "PATH_TO_LABELS = os.path.join('models', MODEL_NAME, LABEL_FILENAME)\n",
    "urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53302420",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore the checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\n",
    "\n",
    "# Load the label map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in the image using the detection model.\"\"\"\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e396c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_detections(frame):\n",
    "    # Convert the frame to a tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, axis=0), dtype=tf.float32)\n",
    "\n",
    "    # Perform object detection\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
